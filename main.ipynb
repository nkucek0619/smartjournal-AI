{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-02-07T19:08:26.257101Z","iopub.status.busy":"2022-02-07T19:08:26.256349Z","iopub.status.idle":"2022-02-07T19:08:26.270934Z","shell.execute_reply":"2022-02-07T19:08:26.270126Z","shell.execute_reply.started":"2022-02-07T19:08:26.257065Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:26.274765Z","iopub.status.busy":"2022-02-07T19:08:26.272416Z","iopub.status.idle":"2022-02-07T19:08:26.282956Z","shell.execute_reply":"2022-02-07T19:08:26.282065Z","shell.execute_reply.started":"2022-02-07T19:08:26.274720Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","\n","\n","from wordcloud import WordCloud\n","import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional,Dropout\n","\n","import re \n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup"]},{"cell_type":"markdown","metadata":{},"source":["# Intro\n","\n","This dataset contains sentences which can be referred to different emotions - sadness, anger, love, surprise, fear and joy.\n","\n","I've tried to do a simple emotion detection by sentence input. \n","\n","In the future I'll upload version 2 to try a different way with more complex modules."]},{"cell_type":"markdown","metadata":{},"source":["# Import data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:26.285096Z","iopub.status.busy":"2022-02-07T19:08:26.284784Z","iopub.status.idle":"2022-02-07T19:08:26.412543Z","shell.execute_reply":"2022-02-07T19:08:26.411727Z","shell.execute_reply.started":"2022-02-07T19:08:26.285055Z"},"trusted":true},"outputs":[],"source":["test_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/test.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\n","train_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/train.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\n","validation_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/val.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\n","print(\"Train : \", train_data.shape)\n","print(\"Test : \", test_data.shape)\n","print(\"Validation : \", validation_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:26.414176Z","iopub.status.busy":"2022-02-07T19:08:26.413720Z","iopub.status.idle":"2022-02-07T19:08:26.426760Z","shell.execute_reply":"2022-02-07T19:08:26.425947Z","shell.execute_reply.started":"2022-02-07T19:08:26.414143Z"},"trusted":true},"outputs":[],"source":["train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["I will first try to see if length of the comments is correlated to the emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:26.429040Z","iopub.status.busy":"2022-02-07T19:08:26.428214Z","iopub.status.idle":"2022-02-07T19:08:26.452570Z","shell.execute_reply":"2022-02-07T19:08:26.451687Z","shell.execute_reply.started":"2022-02-07T19:08:26.428993Z"},"trusted":true},"outputs":[],"source":["train_data['length'] = [len(x) for x in train_data['Comment']]\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:26.454420Z","iopub.status.busy":"2022-02-07T19:08:26.454149Z","iopub.status.idle":"2022-02-07T19:08:27.225815Z","shell.execute_reply":"2022-02-07T19:08:27.225055Z","shell.execute_reply.started":"2022-02-07T19:08:26.454384Z"},"trusted":true},"outputs":[],"source":["all_data = {'Train Data': train_data, 'Validation Data': validation_data, 'Test Data': test_data}\n","fig, ax = plt.subplots(1,3, figsize=(30,10))\n","for i, df in enumerate(all_data.values()):\n","    df2 = df.copy()\n","    df2['length'] = [len(x) for x in df2['Comment']]\n","    sns.kdeplot(data=df2,x='length',hue='Emotion', ax=ax[i])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:27.228125Z","iopub.status.busy":"2022-02-07T19:08:27.227303Z","iopub.status.idle":"2022-02-07T19:08:27.233417Z","shell.execute_reply":"2022-02-07T19:08:27.232357Z","shell.execute_reply.started":"2022-02-07T19:08:27.228082Z"},"trusted":true},"outputs":[],"source":["def words_cloud(wordcloud, df):\n","    plt.figure(figsize=(10, 10))\n","    plt.title(df+' Word Cloud', size = 16)\n","    plt.imshow(wordcloud) \n","    # No axis details\n","    plt.axis(\"off\");"]},{"cell_type":"markdown","metadata":{},"source":["### Words Cloud for each emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:27.235493Z","iopub.status.busy":"2022-02-07T19:08:27.235187Z","iopub.status.idle":"2022-02-07T19:08:27.251076Z","shell.execute_reply":"2022-02-07T19:08:27.250277Z","shell.execute_reply.started":"2022-02-07T19:08:27.235425Z"},"trusted":true},"outputs":[],"source":["emotions_list = train_data['Emotion'].unique()\n","emotions_list"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:27.252730Z","iopub.status.busy":"2022-02-07T19:08:27.252331Z","iopub.status.idle":"2022-02-07T19:08:35.799067Z","shell.execute_reply":"2022-02-07T19:08:35.798321Z","shell.execute_reply.started":"2022-02-07T19:08:27.252692Z"},"trusted":true},"outputs":[],"source":["for emotion in emotions_list:\n","    text = ' '.join([sentence for sentence in train_data.loc[train_data['Emotion'] == emotion,'Comment']])\n","    wordcloud = WordCloud(width = 600, height = 600).generate(text)\n","    words_cloud(wordcloud, emotion)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["#### Label Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:35.801450Z","iopub.status.busy":"2022-02-07T19:08:35.801102Z","iopub.status.idle":"2022-02-07T19:08:35.810908Z","shell.execute_reply":"2022-02-07T19:08:35.810416Z","shell.execute_reply.started":"2022-02-07T19:08:35.801419Z"},"trusted":true},"outputs":[],"source":["lb = LabelEncoder()\n","train_data['Emotion'] = lb.fit_transform(train_data['Emotion'])\n","test_data['Emotion'] = lb.fit_transform(test_data['Emotion'])\n","validation_data['Emotion'] = lb.fit_transform(validation_data['Emotion'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:35.812235Z","iopub.status.busy":"2022-02-07T19:08:35.811888Z","iopub.status.idle":"2022-02-07T19:08:35.829076Z","shell.execute_reply":"2022-02-07T19:08:35.828170Z","shell.execute_reply.started":"2022-02-07T19:08:35.812208Z"},"trusted":true},"outputs":[],"source":["train_data.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["#### Removing unrelevent stopwords and chars\n","Text cleaning fuction was inspired by this project - \n","https://www.kaggle.com/muratkarakurt/emotion-detect-comment-97/notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:35.830749Z","iopub.status.busy":"2022-02-07T19:08:35.830338Z","iopub.status.idle":"2022-02-07T19:08:36.104321Z","shell.execute_reply":"2022-02-07T19:08:36.103429Z","shell.execute_reply.started":"2022-02-07T19:08:35.830707Z"},"trusted":true},"outputs":[],"source":["nltk.download('stopwords')\n","stopwords = set(nltk.corpus.stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:36.105739Z","iopub.status.busy":"2022-02-07T19:08:36.105514Z","iopub.status.idle":"2022-02-07T19:08:36.112201Z","shell.execute_reply":"2022-02-07T19:08:36.111450Z","shell.execute_reply.started":"2022-02-07T19:08:36.105712Z"},"trusted":true},"outputs":[],"source":["max_len=train_data['length'].max()\n","max_len"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(pd.concat(X_train, axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:36.114056Z","iopub.status.busy":"2022-02-07T19:08:36.113606Z","iopub.status.idle":"2022-02-07T19:08:36.122567Z","shell.execute_reply":"2022-02-07T19:08:36.121917Z","shell.execute_reply.started":"2022-02-07T19:08:36.113997Z"},"trusted":true},"outputs":[],"source":["vocabSize = 11000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:49:27.369374Z","iopub.status.busy":"2022-02-07T19:49:27.368773Z","iopub.status.idle":"2022-02-07T19:49:27.377779Z","shell.execute_reply":"2022-02-07T19:49:27.377100Z","shell.execute_reply.started":"2022-02-07T19:49:27.369336Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import one_hot\n","def text_cleaning(df, column):\n","    \"\"\"Removing unrelevent chars, Stemming and padding\"\"\"\n","    stemmer = PorterStemmer()\n","    corpus = []\n","    \n","    for text in df[column]:\n","        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n","        text = text.lower()\n","        text = text.split()\n","        text = [stemmer.stem(word) for word in text if word not in stopwords]\n","        text = \" \".join(text)\n","        corpus.append(text)\n","    one_hot_word = [one_hot(input_text=word, n=vocabSize) for word in corpus]\n","    pad = pad_sequences(sequences=one_hot_word,maxlen=max_len,padding='pre')\n","    print(pad.shape)\n","    return pad"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:49:42.236981Z","iopub.status.busy":"2022-02-07T19:49:42.236675Z","iopub.status.idle":"2022-02-07T19:49:46.508487Z","shell.execute_reply":"2022-02-07T19:49:46.507517Z","shell.execute_reply.started":"2022-02-07T19:49:42.236945Z"},"trusted":true},"outputs":[],"source":["x_train = text_cleaning(train_data, \"Comment\")\n","x_test = text_cleaning(test_data, \"Comment\")\n","x_val = text_cleaning(validation_data, \"Comment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:40.270535Z","iopub.status.busy":"2022-02-07T19:08:40.269965Z","iopub.status.idle":"2022-02-07T19:08:40.275805Z","shell.execute_reply":"2022-02-07T19:08:40.274565Z","shell.execute_reply.started":"2022-02-07T19:08:40.270491Z"},"trusted":true},"outputs":[],"source":["y_train = train_data[\"Emotion\"]\n","y_test = test_data[\"Emotion\"]\n","y_val = validation_data[\"Emotion\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:08:40.277631Z","iopub.status.busy":"2022-02-07T19:08:40.277279Z","iopub.status.idle":"2022-02-07T19:08:40.289124Z","shell.execute_reply":"2022-02-07T19:08:40.288399Z","shell.execute_reply.started":"2022-02-07T19:08:40.277591Z"},"trusted":true},"outputs":[],"source":["y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","y_val = to_categorical(y_val)"]},{"cell_type":"markdown","metadata":{},"source":["# Model building\n","\n","After number of tries with higher/lower dim, and adding layers, this model was the most efficient:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:30:26.893780Z","iopub.status.busy":"2022-02-07T20:30:26.893324Z","iopub.status.idle":"2022-02-07T20:30:27.141888Z","shell.execute_reply":"2022-02-07T20:30:27.141051Z","shell.execute_reply.started":"2022-02-07T20:30:26.893733Z"},"trusted":true},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(input_dim=vocabSize,output_dim=150,input_length=300))\n","model.add(Dropout(0.2))\n","model.add(LSTM(128))\n","model.add(Dropout(0.2))\n","model.add(Dense(64,activation='sigmoid'))\n","model.add(Dropout(0.2))\n","model.add(Dense(6,activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:30:28.600182Z","iopub.status.busy":"2022-02-07T20:30:28.599834Z","iopub.status.idle":"2022-02-07T20:30:28.613812Z","shell.execute_reply":"2022-02-07T20:30:28.612882Z","shell.execute_reply.started":"2022-02-07T20:30:28.600148Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:50:14.506737Z","iopub.status.busy":"2022-02-07T19:50:14.505972Z","iopub.status.idle":"2022-02-07T19:50:14.510575Z","shell.execute_reply":"2022-02-07T19:50:14.509778Z","shell.execute_reply.started":"2022-02-07T19:50:14.506698Z"},"trusted":true},"outputs":[],"source":["callback = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:30:32.658936Z","iopub.status.busy":"2022-02-07T20:30:32.658678Z","iopub.status.idle":"2022-02-07T20:41:43.123291Z","shell.execute_reply":"2022-02-07T20:41:43.122429Z","shell.execute_reply.started":"2022-02-07T20:30:32.658909Z"},"trusted":true},"outputs":[],"source":["hist = model.fit(x_train,y_train,epochs=10,batch_size=64,\n","                 validation_data=(x_val,y_val), verbose=1, callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:43:57.307228Z","iopub.status.busy":"2022-02-07T20:43:57.306905Z","iopub.status.idle":"2022-02-07T20:44:03.076642Z","shell.execute_reply":"2022-02-07T20:44:03.075948Z","shell.execute_reply.started":"2022-02-07T20:43:57.307196Z"},"trusted":true},"outputs":[],"source":["model.evaluate(x_val,y_val,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:44:04.929557Z","iopub.status.busy":"2022-02-07T20:44:04.929268Z","iopub.status.idle":"2022-02-07T20:44:10.706642Z","shell.execute_reply":"2022-02-07T20:44:10.706070Z","shell.execute_reply.started":"2022-02-07T20:44:04.929523Z"},"trusted":true},"outputs":[],"source":["model.evaluate(x_test,y_test,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:44:12.374293Z","iopub.status.busy":"2022-02-07T20:44:12.374045Z","iopub.status.idle":"2022-02-07T20:44:12.740785Z","shell.execute_reply":"2022-02-07T20:44:12.739957Z","shell.execute_reply.started":"2022-02-07T20:44:12.374267Z"},"trusted":true},"outputs":[],"source":["accuracy = hist.history['accuracy']\n","val_acc = hist.history['val_accuracy']\n","loss=hist.history['loss']\n","val_loss=hist.history['val_loss']\n","epochs=range(len(accuracy))\n","\n","plt.plot(epochs,accuracy,'b', label='Training accuracy')\n","plt.plot(epochs,val_acc,'r', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","\n","plt.plot(epochs,loss,'b', label='Training loss')\n","plt.plot(epochs,val_loss,'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T19:21:08.914448Z","iopub.status.busy":"2022-02-07T19:21:08.913977Z","iopub.status.idle":"2022-02-07T19:21:08.921453Z","shell.execute_reply":"2022-02-07T19:21:08.920855Z","shell.execute_reply.started":"2022-02-07T19:21:08.914407Z"},"trusted":true},"outputs":[],"source":["def sentence_cleaning(sentence):\n","    \"\"\"Pre-processing sentence for prediction\"\"\"\n","    stemmer = PorterStemmer()\n","    corpus = []\n","    text = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n","    text = text.lower()\n","    text = text.split()\n","    text = [stemmer.stem(word) for word in text if word not in stopwords]\n","    text = \" \".join(text)\n","    corpus.append(text)\n","    one_hot_word = [one_hot(input_text=word, n=vocabSize) for word in corpus]\n","    pad = pad_sequences(sequences=one_hot_word,maxlen=max_len,padding='pre')\n","    return pad"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-07T20:46:23.690061Z","iopub.status.busy":"2022-02-07T20:46:23.689567Z","iopub.status.idle":"2022-02-07T20:46:24.481669Z","shell.execute_reply":"2022-02-07T20:46:24.480561Z","shell.execute_reply.started":"2022-02-07T20:46:23.690027Z"},"trusted":true},"outputs":[],"source":["sentences = [\n","            \"He was speechles when he found out he was accepted to this new job\",\n","            \"This is outrageous, how can you talk like that?\",\n","            \"I feel like im all alone in this world\",\n","            \"He is really sweet and caring\"\n","            ]\n","for sentence in sentences:\n","    print(sentence)\n","    sentence = sentence_cleaning(sentence)\n","    result = lb.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n","    proba =  np.max(model.predict(sentence))\n","    print(f\"{result} : {proba}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["The model could improve with more playing around with different kind of layers, more complex ones, I assume, but I tried to keep it simple in this project.\n","\n","Overall not bad results."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
